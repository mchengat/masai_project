{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hbol",
   "metadata": {},
   "outputs": [],
   "source": [
    "import marimo as mo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MJUe",
   "metadata": {},
   "source": [
    "# Traffic Congestion Incident Detection Using Machine Learning\n",
    "\n",
    "## Abstract\n",
    "Traffic congestion and unexpected incidents pose significant challenges to urban transportation systems. This project presents a machine learning‚Äìbased approach to detect traffic congestion and road incidents using features derived from aerial camera feeds such as vehicle speed, density, lane occupancy, and queue length. Multiple classification models were evaluated with a primary focus on maximizing recall to avoid missed incidents while maintaining a balanced F1-score. The final model demonstrates stable performance and practical suitability for real-time traffic monitoring systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vblA",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Traffic Congestion Classification\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bkHC",
   "metadata": {},
   "source": [
    "##**1. Problem Formulation**\n",
    "\n",
    "The task is formulated as a binary classification problem, where the objective is to detect traffic congestion or incident conditions. The target variable is defined as:\n",
    "- 0: Normal traffic conditions\n",
    "- 1: Congestion or incident conditions\n",
    "\n",
    "Given the operational impact of missed congestion events, the problem is treated as cost-sensitive, with a higher penalty assigned to false negatives than false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lEQa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------ IMPORTS ----------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import set_config\n",
    "\n",
    "set_config(enable_cython_pairwise_dist=False)\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    StratifiedKFold,\n",
    "    RandomizedSearchCV,\n",
    "    learning_curve,\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    recall_score,\n",
    "    precision_score,\n",
    ")\n",
    "from scipy import stats\n",
    "from scipy.stats import randint, loguniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PKri",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------- LOAD DATA ------------------------------\n",
    "df = pd.read_csv(\"./data/traffic_congestion.csv\")\n",
    "\n",
    "df[\"flow_rate\"] = df[\"vehicle_density\"] * df[\"avg_vehicle_speed\"]\n",
    "X = df.drop(columns=[\"label\"])\n",
    "y = df[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Xref",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SFPL",
   "metadata": {},
   "source": [
    "## 2. Dataset Description\n",
    "\n",
    "1. vehicle_density: The number of vehicles per unit area. High density is a primary indicator of congestion.\n",
    "2. avg_vehicle_speed: The mean speed of all vehicles. Lower speeds directly correlate with increased congestion levels.\n",
    "3. speed_std: Standard deviation of speed. High values indicate \"stop-and-go\" traffic, which is typical during high-congestion periods.\n",
    "4. lane_occupancy: The percentage of time a specific point on the road is occupied by a vehicle. Higher occupancy suggests saturated road capacity.\n",
    "5. queue_length: The length of the line of stationary or slow-moving vehicles. Longer queues indicate bottlenecks or incident-related backups.\n",
    "6. edge_density: A computer-vision metric representing the intensity of edges in an image; more edges typically signify the presence of more vehicles.\n",
    "7. optical_flow_mag: Measures the magnitude of motion between video frames. Low magnitude combined with high density indicates a \"gridlock\" state.\n",
    "8. shadow_fraction: The portion of the scene covered by shadows. While environmental, it can impact sensor accuracy and reflects the time of day.\n",
    "9. time_of_day_norm: Normalized time. Traffic patterns are highly cyclic (e.g., morning and evening rush hours).\n",
    "10. road_width_norm: Normalized width of the segment. Narrower roads (bottlenecks) have lower capacity and higher congestion risk.\n",
    "\n",
    "\n",
    "In traffic congestion and incident detection, the primary goal of the system is to identify abnormal or unsafe traffic conditions as early and accurately as possible. Unlike generic classification tasks, the cost of different types of errors is not equal in this domain. For this reason, Recall and F1-score are more appropriate evaluation metrics than accuracy.\n",
    "\n",
    "### Target Label\n",
    "- Binary classification:\n",
    "  - `0` ‚Üí Normal traffic\n",
    "  - `1` ‚Üí Congestion / incident condition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BYtC",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RGSE",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the class distribution of target column\n",
    "df[\"label\"].value_counts(normalize=True, dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Kclp",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emfo",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Hstk",
   "metadata": {},
   "source": [
    "### Above statistics shows -\n",
    "1. Dataset size consistent (4000 samples)\n",
    "   * No missing values in feature measurements\n",
    "\n",
    "2. Feature behavior insights\n",
    "\n",
    "   * Average Vehicle Speed\n",
    "     Vehicle speed ranges from very low values (~5 km/h) to high free-flow speeds (~90 km/h), with a moderate mean. A strong inverse indicator of congestion\n",
    "\n",
    "   * Vehicle Density\n",
    "      Vehicle density shows high variability, with a relatively low median but a large maximum value. This long-tailed distribution suggests that severe congestion cases are present but less frequent. Density is expected to be a primary contributor to congestion classification.\n",
    "\n",
    "   * Lane Occupancy\n",
    "     Lane occupancy values are bounded between 0 and 1, indicating they are already normalized. The median occupancy is relatively low, while higher values correspond to congested or incident conditions\n",
    "\n",
    "   * Queue Length\n",
    "     Queue length exhibits right-skewed behavior, with most samples having short queues and a smaller number of samples showing very long queues\n",
    "\n",
    "   * Speed Variability\n",
    "     Speed standard deviation displays significant spread, with higher values indicating unstable traffic flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nWHF",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot: All features vs each other\n",
    "corr = df.corr(numeric_only=True)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(\n",
    "    corr,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"coolwarm\",\n",
    "    center=0,\n",
    "    square=True,\n",
    "    linewidths=0.5,\n",
    ")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.yticks(rotation=0)\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iLit",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"label\"\n",
    "corr_target = df.corr(numeric_only=True)[target].sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.barplot(x=corr_target.values, y=corr_target.index)\n",
    "plt.title(f\"Correlation with target: {target}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZHCJ",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "**Feature Engineering and Categorization**\n",
    "The input features are derived from heterogeneous sources and can be grouped into four categories:\n",
    "    1. Traffic State Features\n",
    "        - Vehicle density\n",
    "        - Average vehicle speed\n",
    "        - Speed standard deviation\n",
    "        - Lane occupancy\n",
    "        - Queue length\n",
    "\n",
    "        These features directly reflect macroscopic traffic flow characteristics and are expected to provide the strongest signal for congestion detection.\n",
    "\n",
    "    2. Vision-Based Features\n",
    "        - Optical flow magnitude\n",
    "        - Edge density\n",
    "\n",
    "        These features capture motion and visual clutter from video streams, helping to identify stalled or slow-moving traffic\n",
    "\n",
    "    3. Environmental and Illumination Features\n",
    "\n",
    "        - Shadow fraction\n",
    "\n",
    "        This feature accounts for lighting variations and shadow artifacts that can affect visual feature reliability.\n",
    "\n",
    "    4. Contextual Normalization Features\n",
    "        - Time-of-day normalization\n",
    "        - Road-width normalization\n",
    "\n",
    "        These features provide contextual information that improves generalization across different road geometries and temporal traffic patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ROlb",
   "metadata": {},
   "source": [
    "### Analyze data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qnkX",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_check = [\n",
    "    \"vehicle_density\",\n",
    "    \"avg_vehicle_speed\",\n",
    "    \"lane_occupancy\",\n",
    "    \"queue_length\",\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "for i, feature in enumerate(features_to_check):\n",
    "    # Histogram\n",
    "    plt.subplot(4, 2, 2 * i + 1)\n",
    "    sns.histplot(df[feature], kde=True, color=\"skyblue\", alpha=0.6)\n",
    "    plt.title(f\"Distribution of feature {feature}\")\n",
    "\n",
    "    plt.subplot(4, 2, 2 * i + 2)\n",
    "    stats.probplot(df[feature], dist=\"norm\", plot=plt)\n",
    "    plt.title(f\"Q-Q Plot of {feature}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TqIu",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ TRAIN / TEST SPLIT ------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Vxnm",
   "metadata": {},
   "source": [
    "##4. Feature Scaling\n",
    "The dataset contains features with heterogeneous scales (e.g., speed in km/h, density as counts, occupancy as ratios). Without proper scaling, features with larger numeric ranges may disproportionately influence model training, particularly in distance-based and gradient-based learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DnEU",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- FEATURE GROUPS ---------------------------\n",
    "count_features = [\"vehicle_density\", \"queue_length\"]\n",
    "continuous_features = [\"avg_vehicle_speed\", \"speed_std\"]\n",
    "normalized_features = [\"lane_occupancy\"]\n",
    "\n",
    "# ------------------ PREPROCESSING -----------------------------\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"robust\", RobustScaler(), count_features),\n",
    "        (\"standard\", StandardScaler(), continuous_features),\n",
    "        (\"passthrough\", \"passthrough\", normalized_features),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ulZA",
   "metadata": {},
   "source": [
    "## 5. Model Selection and Hyperparameter Tuning\n",
    "Multiple supervised learning models were evaluated to identify the most suitable approach for traffic congestion incident detection.\n",
    "\n",
    "The tuning process focused on:\n",
    "- Maximizing recall without severely degrading precision\n",
    "- Reducing overfitting and model variance\n",
    "- Identifying stable hyperparameter regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfG",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ STRATIFIED CV -----------------------------\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# ------------------ MODEL PIPELINES ---------------------------\n",
    "knn_pipeline = Pipeline(\n",
    "    steps=[(\"preprocessing\", preprocessor), (\"clf\", KNeighborsClassifier())]\n",
    ")\n",
    "\n",
    "svm_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessing\", preprocessor),\n",
    "        (\"clf\", SVC(kernel=\"rbf\", probability=True, class_weight=\"balanced\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "rf_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessing\", preprocessor),\n",
    "        (\n",
    "            \"clf\",\n",
    "            RandomForestClassifier(\n",
    "                class_weight=\"balanced\",\n",
    "                random_state=42,\n",
    "                n_jobs=1,  # IMPORTANT on M1\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "logreg_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessing\", preprocessor),\n",
    "        (\n",
    "            \"clf\",\n",
    "            LogisticRegression(\n",
    "                class_weight=\"balanced\",\n",
    "                max_iter=2000,\n",
    "                solver=\"saga\",\n",
    "                l1_ratio=0.0,\n",
    "                C=1.0,\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ---------------- PARAMETER DISTRIBUTIONS ---------------------\n",
    "knn_param_dist = {\n",
    "    \"clf__n_neighbors\": range(1, 50, 2),\n",
    "    \"clf__weights\": [\"uniform\", \"distance\"],\n",
    "    \"clf__metric\": [\"euclidean\", \"manhattan\"],\n",
    "}\n",
    "\n",
    "svm_param_dist = {\n",
    "    \"clf__C\": loguniform(1e-2, 1e2),\n",
    "    \"clf__gamma\": loguniform(1e-3, 1),\n",
    "}\n",
    "\n",
    "rf_param_dist = {\n",
    "    \"clf__n_estimators\": [150, 200, 300, 400, 500],\n",
    "    \"clf__max_depth\": [6, 8, 10, 12, 14],\n",
    "    \"clf__min_samples_leaf\": [10, 15, 20, 25],\n",
    "    \"clf__min_samples_split\": [20, 30, 40],\n",
    "    \"clf__max_features\": [\"sqrt\", 0.5],\n",
    "}\n",
    "\n",
    "logreg_param_dist = {\n",
    "    # \"clf__C\": loguniform(1e-3, 1e2),\n",
    "    # \"clf__l1_ratio\": [0.0],\n",
    "    \"clf__C\": [1e-3, 1e-2, 1e-1, 0.5, 1.0, 2.0, 5.0, 10.0],\n",
    "    \"clf__solver\": [\"saga\"],\n",
    "    \"clf__l1_ratio\": [0.0, 0.25, 0.5, 0.75, 1.0],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Pvdt",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, X, y, title, cv, scoring=\"f1\", n_jobs=-1):\n",
    "    metrics = [\"f1\", \"recall\"]\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=1, ncols=2, figsize=(14, 5), sharex=True, sharey=False\n",
    "    )\n",
    "\n",
    "    for ax, scoring in zip(axes, metrics):\n",
    "        train_sizes, train_scores, val_scores = learning_curve(\n",
    "            estimator=estimator,\n",
    "            X=X,\n",
    "            y=y,\n",
    "            train_sizes=np.linspace(0.1, 1.0, 8),\n",
    "            cv=cv,\n",
    "            scoring=scoring,\n",
    "            n_jobs=n_jobs,\n",
    "            shuffle=True,\n",
    "            random_state=42,\n",
    "        )\n",
    "\n",
    "        train_mean = train_scores.mean(axis=1)\n",
    "        train_std = train_scores.std(axis=1)\n",
    "        val_mean = val_scores.mean(axis=1)\n",
    "        val_std = val_scores.std(axis=1)\n",
    "\n",
    "        ax.plot(train_sizes, train_mean, \"o-\", label=\"Training score\")\n",
    "        ax.plot(train_sizes, val_mean, \"o-\", label=\"Cross-validation score\")\n",
    "\n",
    "        ax.fill_between(\n",
    "            train_sizes,\n",
    "            train_mean - train_std,\n",
    "            train_mean + train_std,\n",
    "            alpha=0.2,\n",
    "        )\n",
    "        ax.fill_between(\n",
    "            train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.2\n",
    "        )\n",
    "\n",
    "        ax.set_title(f\"{title} ({scoring.upper()})\")\n",
    "        ax.set_xlabel(\"Training Set Size\")\n",
    "        ax.set_ylabel(scoring.upper())\n",
    "        ax.grid(alpha=0.3)\n",
    "        ax.legend(loc=\"best\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZBYS",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- RANDOM SEARCH FUNCTION ----------------------\n",
    "def run_random_search(pipeline, param_dist, model_name, n_iter):\n",
    "    print(f\"\\n===== RandomizedSearchCV: {model_name} =====\")\n",
    "\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=n_iter,\n",
    "        scoring={\"f1\": \"f1\", \"recall\": \"recall\"},\n",
    "        refit=\"recall\",\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    search.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters:\", search.best_params_)\n",
    "    print(\"Best CV F1 score:\", search.best_score_)\n",
    "\n",
    "    return (\n",
    "        search.best_estimator_,\n",
    "        pd.DataFrame(search.cv_results_),\n",
    "        search.best_params_,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aLJB",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- TRAIN MODELS --------------------------------\n",
    "best_knn, results_knn, params_knn = run_random_search(\n",
    "    knn_pipeline, knn_param_dist, \"KNN\", n_iter=20\n",
    ")\n",
    "best_svm, results_svm, params_svm = run_random_search(\n",
    "    svm_pipeline, svm_param_dist, \"SVM\", n_iter=20\n",
    ")\n",
    "best_rf, results_rf, params_rf = run_random_search(\n",
    "    rf_pipeline, rf_param_dist, \"Random Forest\", n_iter=20\n",
    ")\n",
    "best_logreg, results_logreg, params_logreg = run_random_search(\n",
    "    logreg_pipeline, logreg_param_dist, \"Logistic Regression\", n_iter=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nHfw",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation\n",
    "\n",
    "Model performance was evaluated using multiple metrics and visualization techniques.\n",
    "\n",
    "### Evaluation Metrics\n",
    "- Recall\n",
    "- Precision\n",
    "- F1-score\n",
    "- ROC-AUC\n",
    "\n",
    "### Diagnostic Tools\n",
    "- Confusion Matrix\n",
    "- ROC Curve\n",
    "- Learning Curves (training vs validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xXTn",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================Learning Curves==================#\n",
    "plot_learning_curve(\n",
    "    best_rf, X_train, y_train, title=\"Learning Curve ‚Äì Random Forest\", cv=cv\n",
    ")\n",
    "\n",
    "plot_learning_curve(\n",
    "    best_logreg,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    title=\"Learning Curve ‚Äì Logistic Regression\",\n",
    "    cv=cv,\n",
    ")\n",
    "\n",
    "plot_learning_curve(\n",
    "    best_svm, X_train, y_train, title=\"Learning Curve ‚Äì SVM\", cv=cv\n",
    ")\n",
    "\n",
    "plot_learning_curve(\n",
    "    best_knn, X_train, y_train, title=\"Learning Curve ‚Äì KNN\", cv=cv\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AjVT",
   "metadata": {},
   "source": [
    "### Learning curve Analysis\n",
    "\n",
    "**Logistic regression**: The learning curves show that logistic regression performs consistently, with similar training and validation F1-score and recall across different training sizes. This indicates that the model generalizes well without overfitting, making it a dependable choice for traffic congestion detection, especially in safety-critical applications.\n",
    "\n",
    "**Random Forest**: The learning curves show that the Random Forest model performs very well on the training data but less effectively on the validation data, indicating overfitting. Although adding more training data slightly improves performance, the gap between training and validation results remains. This suggests that improving the model through better tuning and regularization is more effective than collecting additional data.\n",
    "\n",
    "**SVM**: The SVM learning curves demonstrate rapid convergence and strong generalization, with training and validation F1-score and recall closely aligned after approximately 600 samples. The model achieves higher recall than logistic regression while maintaining low variance, indicating that it effectively captures non-linear congestion patterns without overfitting.\n",
    "\n",
    "**KNN**: is massively overfitting and not suitable as a final model for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pHFh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============Random Forest==================== #\n",
    "\n",
    "mean_scores_by_trees = (\n",
    "    results_rf.groupby(\"param_clf__n_estimators\")[\n",
    "        [\"mean_test_f1\", \"mean_test_recall\"]\n",
    "    ]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(\n",
    "    mean_scores_by_trees[\"param_clf__n_estimators\"],\n",
    "    mean_scores_by_trees[\"mean_test_recall\"],\n",
    "    marker=\"o\",\n",
    "    label=\"Recall\",\n",
    ")\n",
    "plt.plot(\n",
    "    mean_scores_by_trees[\"param_clf__n_estimators\"],\n",
    "    mean_scores_by_trees[\"mean_test_f1\"],\n",
    "    marker=\"s\",\n",
    "    label=\"F1-score\",\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Number of Trees\")\n",
    "plt.ylabel(\"CV Score\")\n",
    "plt.title(\"Random Forest: Recall & F1 vs Number of Trees\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =================================================== #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NCOB",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============Logistic Reg==================== #\n",
    "\n",
    "mean_scores_by_log_C = (\n",
    "    results_logreg.groupby(\"param_clf__C\")[\n",
    "        [\"mean_test_f1\", \"mean_test_recall\"]\n",
    "    ]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .sort_values(\"param_clf__C\")\n",
    ")\n",
    "\n",
    "# print(mean_scores_by_C)\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.semilogx(\n",
    "    mean_scores_by_log_C[\"param_clf__C\"],\n",
    "    mean_scores_by_log_C[\"mean_test_recall\"],\n",
    "    marker=\"o\",\n",
    "    label=\"Recall\",\n",
    ")\n",
    "plt.semilogx(\n",
    "    mean_scores_by_log_C[\"param_clf__C\"],\n",
    "    mean_scores_by_log_C[\"mean_test_f1\"],\n",
    "    marker=\"s\",\n",
    "    label=\"F1-score\",\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Regularization Strength (C)\")\n",
    "plt.ylabel(\"CV Score\")\n",
    "plt.title(\"Logistic Regression: Recall & F1 vs C\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# =================================================== #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aqbW",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============SVM==================== #\n",
    "\n",
    "mean_scores_by_svm_C = (\n",
    "    results_svm.groupby(\"param_clf__C\")[[\"mean_test_f1\", \"mean_test_recall\"]]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .sort_values(\"param_clf__C\")\n",
    ")\n",
    "\n",
    "# print(mean_scores_by_C)\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.semilogx(\n",
    "    mean_scores_by_svm_C[\"param_clf__C\"],\n",
    "    mean_scores_by_svm_C[\"mean_test_recall\"],\n",
    "    marker=\"o\",\n",
    "    label=\"Recall\",\n",
    ")\n",
    "plt.semilogx(\n",
    "    mean_scores_by_svm_C[\"param_clf__C\"],\n",
    "    mean_scores_by_svm_C[\"mean_test_f1\"],\n",
    "    marker=\"s\",\n",
    "    label=\"F1-score\",\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Regularization Strength (C)\")\n",
    "plt.ylabel(\"CV Score\")\n",
    "plt.title(\"SVM: Recall & F1 vs C\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# =================================================== #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TRpd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============KNN==================== #\n",
    "\n",
    "mean_scores_by_k = (\n",
    "    results_knn.groupby(\"param_clf__n_neighbors\")[\n",
    "        [\"mean_test_f1\", \"mean_test_recall\"]\n",
    "    ]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# print(mean_scores_by_k)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.semilogx(\n",
    "    mean_scores_by_k[\"param_clf__n_neighbors\"],\n",
    "    mean_scores_by_k[\"mean_test_recall\"],\n",
    "    marker=\"o\",\n",
    "    label=\"Recall\",\n",
    ")\n",
    "plt.semilogx(\n",
    "    mean_scores_by_k[\"param_clf__n_neighbors\"],\n",
    "    mean_scores_by_k[\"mean_test_f1\"],\n",
    "    marker=\"s\",\n",
    "    label=\"F1-score\",\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Number of Neighbors\")\n",
    "plt.ylabel(\"CV Score\")\n",
    "plt.title(\"KNN: Recall & F1 vs C\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# =================================================== #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TXez",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- EVALUATION FUNCTION -------------------------\n",
    "def evaluate_model(model, model_name):\n",
    "    print(f\"\\n===== Evaluation: {model_name} =====\")\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Recall Score\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    print(\"Recall Score:\", recall)\n",
    "\n",
    "    # F1 Score\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    print(\"F1 Score:\", f1)\n",
    "\n",
    "    # Precision\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    print(\"Precision:\", precision)\n",
    "\n",
    "    # Classification Report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Create figure with 1 row, 2 columns\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # -------------------------\n",
    "    # Confusion Matrix (Left)\n",
    "    # -------------------------\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot(ax=axes[0], colorbar=False)\n",
    "    axes[0].set_title(f\"{model_name} ‚Äì Confusion Matrix\")\n",
    "\n",
    "    # -------------------------\n",
    "    # ROC Curve (Right)\n",
    "    # -------------------------\n",
    "    roc_auc = roc_auc_score(y_test, y_proba)\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "\n",
    "    axes[1].plot(fpr, tpr, label=f\"ROC-AUC = {roc_auc:.3f}\")\n",
    "    axes[1].plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
    "    axes[1].set_xlabel(\"False Positive Rate\")\n",
    "    axes[1].set_ylabel(\"True Positive Rate\")\n",
    "    axes[1].set_title(f\"{model_name} ‚Äì ROC Curve\")\n",
    "    axes[1].legend(loc=\"lower right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return {\n",
    "        \"Model\": model_name,\n",
    "        \"Recall\": recall,\n",
    "        \"F1\": f1,\n",
    "        \"Precision\": precision,\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------------- FINAL EVALUATION ----------------------------\n",
    "results = []\n",
    "results.append(evaluate_model(best_logreg, \"Logistic Regression\"))\n",
    "results.append(evaluate_model(best_knn, \"KNN\"))\n",
    "results.append(evaluate_model(best_svm, \"SVM\"))\n",
    "results.append(evaluate_model(best_rf, \"Random Forest\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dNNg",
   "metadata": {},
   "source": [
    "### Findings on Recall & F1 Score\n",
    "\n",
    "- KNN: Is highly sensitive to the choice of neighbors and does not achieve competitive recall, making it unsuitable as a final mode\n",
    "- SVM : SVM achieves strong and stable recall after tuning, making it one of the most reliable models for congestion detection.\n",
    "- Logistic Regression: Logistic Regression is stable, easy to tune, and generalizes well, making it a strong baseline and a safe deployment choice.\n",
    "- Random Forest: Random Forest shows diminishing returns with more trees and requires structural regularization rather than more estimators.\n",
    "\n",
    "The hyperparameter analysis shows that Logistic Regression and SVM provide stable and reliable performance for traffic congestion detection. Logistic Regression offers strong generalization and simplicity, while SVM achieves the highest recall with consistent performance after tuning. In contrast, KNN suffers from overfitting and instability, and Random Forest shows limited improvement despite increased model complexity. Based on these findings, SVM is selected as the best-performing model, with Logistic Regression serving as a robust baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yCnT",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "best_model_row = results_df.loc[results_df[\"Recall\"].idxmax()]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(f\"üèÜ BEST PIPELINE: {best_model_row['Model']}\")\n",
    "print(f\"   Recall: {best_model_row['Recall']:.2%}\")\n",
    "print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wlCL",
   "metadata": {},
   "source": [
    "## 7. Traffic Congestion Heatmap Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kqZH",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add predicted congestion probability\n",
    "df_vis = df.copy()\n",
    "df_vis[\"congestion_prob\"] = best_logreg.predict_proba(X)[:, 1]\n",
    "\n",
    "# Bin traffic state variables\n",
    "df_vis[\"occupancy_bin\"] = pd.cut(df_vis[\"lane_occupancy\"], bins=10)\n",
    "df_vis[\"queue_bin\"] = pd.cut(df_vis[\"queue_length\"], bins=10)\n",
    "\n",
    "# Aggregate congestion probability\n",
    "heatmap_data = (\n",
    "    df_vis.groupby([\"occupancy_bin\", \"queue_bin\"])[\"congestion_prob\"]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Pivot for heatmap\n",
    "pivot = heatmap_data.pivot(\n",
    "    index=\"occupancy_bin\", columns=\"queue_bin\", values=\"congestion_prob\"\n",
    ")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "ax = sns.heatmap(\n",
    "    pivot, cmap=\"hot\", cbar_kws={\"label\": \"Average Congestion Probability\"}\n",
    ")\n",
    "# invert y-axis so small->large goes bottom->top\n",
    "ax.invert_yaxis()\n",
    "plt.title(\"Traffic Congestion Heatmap (Lane Occupancy vs Queue Length)\")\n",
    "plt.xlabel(\"Queue Length\")\n",
    "plt.ylabel(\"Lane Occupancy\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wAgl",
   "metadata": {},
   "source": [
    "- Dark red / black ‚Üí Low congestion probability\n",
    "- Orange / yellow ‚Üí Medium congestion probability\n",
    "- White ‚Üí Very high congestion probability (‚âà 1.0)\n",
    "\n",
    "The highest congestion risk is concentrated in the top-right region of the plot, where lane occupancy is high and queue lengths are long, indicating severe congestion conditions caused by roadway capacity saturation and queue spillbac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rEll",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vis[\"density_bin\"] = pd.cut(df_vis[\"vehicle_density\"], bins=10)\n",
    "df_vis[\"speed_bin\"] = pd.cut(df_vis[\"avg_vehicle_speed\"], bins=10)\n",
    "\n",
    "heatmap_data_1 = (\n",
    "    df_vis.groupby([\"density_bin\", \"speed_bin\"])[\"congestion_prob\"]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "pivot_1 = heatmap_data_1.pivot(\n",
    "    index=\"density_bin\", columns=\"speed_bin\", values=\"congestion_prob\"\n",
    ")\n",
    "\n",
    "ax2 = sns.heatmap(pivot_1, cmap=\"hot\")\n",
    "ax2.invert_yaxis()\n",
    "plt.title(\"Traffic Congestion Heatmap (Density vs Speed)\")\n",
    "plt.xlabel(\"Average Speed\")\n",
    "plt.ylabel(\"Vehicle Density\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dGlV",
   "metadata": {},
   "source": [
    "Above heatmap visualizes the average predicted congestion probability across different combinations of vehicle density (y-axis) and average traffic speed (x-axis). Dark colors represent low congestion, while brighter (yellow to white) regions indicate high congestion risk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SdmI",
   "metadata": {},
   "source": [
    "##8. Probability Threshold Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lgWD",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the trained logistic regression pipeline\n",
    "# (assumes variable name: best_pipeline or logreg_pipeline)\n",
    "\n",
    "# Predict probabilities\n",
    "y_prob = best_svm.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Apply LOWER threshold (35%)\n",
    "threshold = 0.40\n",
    "y_pred_thresh = (y_prob >= threshold).astype(int)\n",
    "\n",
    "# Evaluate\n",
    "print(\"SVM\")\n",
    "print(f\"Decision Threshold: {threshold}\")\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_thresh))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_thresh))\n",
    "print(\"F1-score:\", f1_score(y_test, y_pred_thresh))\n",
    "\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred_thresh))\n",
    "\n",
    "\n",
    "y_prob = best_logreg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Apply LOWER threshold (35%)\n",
    "y_pred_thresh = (y_prob >= threshold).astype(int)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Decision Threshold: {threshold}\")\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_thresh))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_thresh))\n",
    "print(\"F1-score:\", f1_score(y_test, y_pred_thresh))\n",
    "\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred_thresh))\n",
    "\n",
    "y_prob = best_knn.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Apply LOWER threshold (35%)\n",
    "y_pred_thresh = (y_prob >= threshold).astype(int)\n",
    "\n",
    "# Evaluate\n",
    "print(\"KNN\")\n",
    "print(f\"Decision Threshold: {threshold}\")\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_thresh))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_thresh))\n",
    "print(\"F1-score:\", f1_score(y_test, y_pred_thresh))\n",
    "\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred_thresh))\n",
    "\n",
    "\n",
    "y_prob = best_rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Apply LOWER threshold (35%)\n",
    "y_pred_thresh = (y_prob >= threshold).astype(int)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Random Forest\")\n",
    "print(f\"Decision Threshold: {threshold}\")\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_thresh))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_thresh))\n",
    "print(\"F1-score:\", f1_score(y_test, y_pred_thresh))\n",
    "\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred_thresh))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yOPj",
   "metadata": {},
   "source": [
    "### Plot decision boundaries for different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fwwy",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2d = df[[\"vehicle_density\", \"avg_vehicle_speed\"]]\n",
    "# We need to scale these 2 features similarly to how they were scaled in full model\n",
    "# In full model: density->Robust, speed->Standard\n",
    "# We'll replicate that manually for the 2D pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "class TwoFeatureScaler(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        self.r_scaler = RobustScaler().fit(X[[\"vehicle_density\"]])\n",
    "        self.s_scaler = StandardScaler().fit(X[[\"avg_vehicle_speed\"]])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        d = self.r_scaler.transform(X[[\"vehicle_density\"]])\n",
    "        s = self.s_scaler.transform(X[[\"avg_vehicle_speed\"]])\n",
    "        return np.hstack([d, s])\n",
    "\n",
    "\n",
    "# Re-instantiate models with best params (stripping 'clf__' prefix)\n",
    "knn_2d = KNeighborsClassifier(\n",
    "    **{k.replace(\"clf__\", \"\"): v for k, v in params_knn.items()}\n",
    ")\n",
    "svm_2d = SVC(\n",
    "    probability=True,\n",
    "    class_weight=\"balanced\",\n",
    "    **{k.replace(\"clf__\", \"\"): v for k, v in params_svm.items()},\n",
    ")\n",
    "rf_2d = RandomForestClassifier(\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=42,\n",
    "    **{k.replace(\"clf__\", \"\"): v for k, v in params_rf.items()},\n",
    ")\n",
    "logreg_2d = LogisticRegression(\n",
    "    class_weight=\"balanced\",\n",
    "    max_iter=2000,\n",
    "    **{k.replace(\"clf__\", \"\"): v for k, v in params_logreg.items()},\n",
    ")\n",
    "\n",
    "models_2d = [\n",
    "    (\"KNN\", knn_2d),\n",
    "    (\"SVM\", svm_2d),\n",
    "    (\"Random Forest\", rf_2d),\n",
    "    (\"Logistic Regression\", logreg_2d),\n",
    "]\n",
    "\n",
    "# Train and Plot\n",
    "plt.figure(figsize=(15, 12))\n",
    "scaler_2d = TwoFeatureScaler()\n",
    "X_2d_scaled = scaler_2d.fit_transform(X_2d)\n",
    "y_2d = y.values\n",
    "\n",
    "# Meshgrid\n",
    "h = 0.02\n",
    "x_min, x_max = X_2d_scaled[:, 0].min() - 0.5, X_2d_scaled[:, 0].max() + 0.5\n",
    "y_min, y_max = X_2d_scaled[:, 1].min() - 0.5, X_2d_scaled[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "for idx1, (name, model) in enumerate(models_2d):\n",
    "    model.fit(X_2d_scaled, y_2d)\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.subplot(2, 2, idx1 + 1)\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap=\"coolwarm\")\n",
    "    sns.scatterplot(\n",
    "        x=X_2d_scaled[:, 0],\n",
    "        y=X_2d_scaled[:, 1],\n",
    "        hue=y_2d,\n",
    "        palette={0: \"blue\", 1: \"red\"},\n",
    "        alpha=0.5,\n",
    "        s=20,\n",
    "        legend=False,\n",
    "    )\n",
    "    plt.title(f\"{name} (Best Params)\")\n",
    "    plt.xlabel(\"Density (Scaled)\")\n",
    "    plt.ylabel(\"Speed (Scaled)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"best_params_boundaries.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LJZf",
   "metadata": {},
   "source": [
    "## 9. Final Model Selection & Conclusion\n",
    "### Performance Summary\n",
    "\n",
    "The primary objective was to identify a model that **maximizes Recall** (to ensure no accidents/incidents are missed) while maintaining a **high F1-score** (to minimize false alarms and avoid *alert fatigue* for traffic operators).\n",
    "\n",
    "### Model Comparison\n",
    "\n",
    "| Model | Peak Recall | Stability | Recommendation |\n",
    "|------|------------|-----------|----------------|\n",
    "| **Logistic Regression** | ~84% | High | **Selected Model** |\n",
    "| **SVM** | ~86% (at `C = 0.01`) | Low | Secondary / Risky |\n",
    "| **Random Forest** | ~83% | Moderate | Overfits Training Data |\n",
    "| **KNN** | ~80% | High | Underperforms |\n",
    "\n",
    "---\n",
    "\n",
    "### Detailed Model Evaluation\n",
    "\n",
    "#### A. The Selection: Logistic Regression\n",
    "\n",
    "Logistic Regression emerged as the most robust candidate.\n",
    "\n",
    "- **Convergence**:\n",
    "  The learning curves for both F1-score and Recall show training and cross-validation scores converging tightly at approximately **2,500 samples**, indicating a **low-variance model** that generalizes well.\n",
    "\n",
    "- **Optimal Hyperparameters**:\n",
    "  Stable performance is observed for `C ‚â• 10‚Åª¬π`, making the model less sensitive to small tuning changes.\n",
    "\n",
    "---\n",
    "\n",
    "#### B. The SVM Anomaly (High Recall vs. High Variance)\n",
    "\n",
    "While SVM achieved a peak Recall of **86% at `C = 0.01`**, it was not selected as the primary model due to:\n",
    "\n",
    "- **Extreme Sensitivity**:\n",
    "  A small change in the regularization parameter (`C`) caused Recall to drop sharply from **86% to 65%**.\n",
    "\n",
    "- **Information Gap**:\n",
    "  At the point of highest Recall, the variance (shown by the shaded region in the learning curve) was at its maximum, indicating **unpredictable performance** across different traffic data subsets.\n",
    "\n",
    "---\n",
    "\n",
    "#### C. Overfitting in Random Forest\n",
    "\n",
    "Random Forest achieved a competitive Recall of approximately **83%**, but the learning curve revealed a persistent gap between training and validation scores.\n",
    "\n",
    "- This behavior indicates **overfitting**, where the model memorizes specific traffic patterns rather than learning general congestion characteristics.\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "For **Traffic Congestion Incident Detection**, **Logistic Regression** is recommended.\n",
    "\n",
    "#### Justification\n",
    "\n",
    "- **Safety First**:\n",
    "  Achieves a reliable **84% Recall**, ensuring most critical incidents are detected.\n",
    "\n",
    "- **Operational Efficiency**:\n",
    "  Computationally lightweight, enabling **sub-second inference** on live traffic sensor data.\n",
    "\n",
    "- **Consistency**:\n",
    "  Unlike SVM, it maintains stable performance across different training sizes and hyperparameter se"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "urSm",
   "metadata": {},
   "source": [
    "## 12. Future Work\n",
    "\n",
    "Future improvements may include:\n",
    "- Incorporating spatial data (GPS / camera locations)\n",
    "- Temporal modeling using time-series or deep learning approaches\n",
    "- Adaptive thresholding based on traffic conditions\n",
    "- Integration with live traffic management systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jxvo",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "marimo": {
   "app_config": {
    "width": "medium"
   },
   "marimo_version": "0.19.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
